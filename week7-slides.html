<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 7 - Text analysis (2)</title>
    <meta charset="utf-8" />
    <meta name="author" content="Iain Emsley, Carlos Cámara-Menoyo" />
    <script src="week7-slides_files/header-attrs-2.11/header-attrs.js"></script>
    <link href="week7-slides_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="week7-slides_files/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="week7-slides_files/tile-view-0.2.6/tile-view.js"></script>
    <script src="week7-slides_files/js-cookie-3.0.0/js.cookie.js"></script>
    <script src="week7-slides_files/peerjs-1.3.1/peerjs.min.js"></script>
    <script src="week7-slides_files/tiny.toast-1.0.0/toast.min.js"></script>
    <link href="week7-slides_files/xaringanExtra-broadcast-0.2.6/broadcast.css" rel="stylesheet" />
    <script src="week7-slides_files/xaringanExtra-broadcast-0.2.6/broadcast.js"></script>
    <script src="week7-slides_files/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="week7-slides_files/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="week7-slides_files/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <script src="week7-slides_files/xaringanExtra_fit-screen-0.2.6/fit-screen.js"></script>
    <link href="week7-slides_files/tachyons-4.12.0/tachyons.min.css" rel="stylesheet" />
    <link href="week7-slides_files/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="week7-slides_files/panelset-0.2.6/panelset.js"></script>
    <script src="week7-slides_files/xaringanExtra-progressBar-0.0.1/progress-bar.js"></script>
    <link rel="stylesheet" href="week7-slides_files/sass5da3691707b7c6f13685fdf276a48fe1/warwick.min.css" type="text/css" />
    <link rel="stylesheet" href="week7-slides_files/sass3273d0e383f1ce432e8e6e82308d0fbd/warwick-fonts.min.css" type="text/css" />
    <link rel="stylesheet" href="widths.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Week 7 - Text analysis (2)
## IM904: Digital Objects, Digital Methods
### Iain Emsley, Carlos Cámara-Menoyo
### Centre for Interdisciplinary Methodologies

---


<style>.xe__progress-bar__container {
  bottom:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #552D62;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>

## Previously...

Here we build on the last text analysis lab. You have used text analysis to identify patterns in our data  using techniques such as ngrams, and bigrams in particular, and  to explore language through frequently occurring terms and used visualisation as an  exploratory method. 

---

## Text analysis 2: Topic Modelling

In this, we turn to purely .highlight[algorithmic methods] that are becoming more common. We will use .highlight[**Topic Modelling**: an unsupervised machine learning to automate our reading of the data]. Instead of guiding the model by tagging data, we let the algorithm identify what we should attend to by identifying points of density between words (as clusters) and then between densities of documents that share these clusters. When we read a topic model, we are not reading the texts but the statistical model of the texts. That means that an algorithm focuses on what language to use and discard and, unlike the previous session, you may not know what data has been ignored. You may wonder if this is a sensible idea and that is an excellent opportunity to reflect on algorithmic criticism. 

Here we begin to use Topic Modelling to take high level approaches to our data and to think about the patterns that might be found in it. Like the first text analysis session and networks, we cannot hope to cover the topics in full in this lab but it should provide enough information to reflect on these approaches. I hope that we can have a group discussion at the end to reflect on the issues raised in this lab and what it means for digital methods presented here and in the previous sessions. 

In this lab, we are going to use topic modelling to reflect on two papers, Thelwall's _Social Media for Youtube_ (2018) and Beytia and Wagner's _Visibility Layers_ (2020). We explore the idea of .highlight[topical asymmetry] from Beytia and Wagner through topic modelling and by using the critical questions as to how to the corpus is created using James Dobson's critique of topic modelling (2019).

---

## Learning Objectives

By the end of this lesson, you will have a basic understanding of:

* topic modelling;
* critical approaches to topic modelling;
* the differences and similarities of algorithmic versus computational reading. 

---

layout: true
## Requirements

---
Before we get started, we need to set our environment up. We will need some new packages to handle the topic modelling on top of the ones loaded for the last session. 


```r
install.packages('dplyr')
install.packages('tidytext')
install.packages('forcats')
install.packages('topicmodels')
install.packages('reshape2')
```

.bg-washed-red.b--dark-red.ba.bw2.br3.shadow-5.ph4.mt5[
**Side Note for Homebrew users**:  
If you have installed R using Homebrew, you will need to alter a file to complete the installation of the topicmodels package. The details are on this page:
[https://tinyheero.github.io/2016/02/20/install-r-topicmodels.html](https://tinyheero.github.io/2016/02/20/install-r-topicmodels.html)
]

---


If you have installed the packages, you will not need to do this again. You will need to tell R to use them though. 


```r
library(dplyr)
```

```
## 
## Attaching package: 'dplyr'
```

```
## The following objects are masked from 'package:stats':
## 
##     filter, lag
```

```
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
```

```r
library(tidytext)
library(ggplot2)
library(forcats)
library(topicmodels)
library(stringr)
library(tidyr)
library(reshape2)
```

```
## 
## Attaching package: 'reshape2'
```

```
## The following object is masked from 'package:tidyr':
## 
##     smiths
```

---

We will use our Reddit data again. If you do not have any Reddit data, you can use this file:
&lt;https://files.warwick.ac.uk/iainemsley/files/IM904/reddit_parisagreementclimate_2021-01-27+094516_.csv&gt;
Please change this path to your data.

```r
# Load the data.
dataFile = "data/week2/reddit_COP26_2022-01-18.csv"
reddit_df &lt;- read.csv(dataFile, stringsAsFactors = FALSE, encoding = 'UTF-8')

# Uncomment below to explore the data to see if it's what we wanted to load.
# head(reddit_df)
# View(reddit_df)
```

In this lab, we will be using .highlight[**topic modelling** as the main technique]. We will reflect on the Ben-David et al (2019) paper and use the same algorithm to explore the Reddit data.  

---

layout: false

## Topic Modelling

.pull-left[
We can use topic modelling to identify potential areas and threads of discussion. Topic Models break up the document into a series of categories based on probability. The [Latent Dirichlet Allocation (LDA) algorithm](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (Blei, Ng, and Jordan, 2003) is used to identify potential topics based on word similarity. Topic Modelling applies these topics to the documents and provides a statistical count for the likelihood that this topic is present. ]


.pull-right[
![](https://thatware.co/wp-content/uploads/2020/07/LDA-topic-modeing-1024x515-1.png)
]

---

## What are we processing?


In the first step, **we need to think carefully about what we are modelling**. Here we should think about 

* the .highlight[format of the data that we are using] - is it Reddit, Twitter, Weibo or similar? 
* whether, and how, we might need to clean up our data. -&gt; This depends on the format of the data (remember affordances?)
  * If we use Reddit comments, then we might want to think about how the documents are divided. In the exercise, we will use the Reddit data and you should be attentive to how it is structured: it is subdivided by subreddit (if at all), by post and comment, and by author. 
  * Further processing may have taken place to identify particular posts and the data is grouped by these - think about the questions that Beytia and Wagner raise and what steps might be taken to approach them. Hint: think of what they are asking in the reports in their methods and work it backwards.

---
layout: true
### Data preparation

---

### Stop words

A process that we might undertake is to prepare our data and one step is to **remove stop words**. Stop words are commonly occurring terms like `the`, `a`, `and`, `from` and so on. They are used frequently but may not add meaning to the data. Removing stop words can be tricky. 

R has a list of stop words that can be used and other tools have their own lists of words (i.e. ). 


```r
# Tidytext provides a data frame with 1149 English Stop Words
head(tidytext::stop_words, 15)
```

```
## # A tibble: 15 × 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           SMART  
##  2 a's         SMART  
##  3 able        SMART  
##  4 about       SMART  
##  5 above       SMART  
##  6 according   SMART  
##  7 accordingly SMART  
##  8 across      SMART  
##  9 actually    SMART  
## 10 after       SMART  
## 11 afterwards  SMART  
## 12 again       SMART  
## 13 against     SMART  
## 14 ain't       SMART  
## 15 all         SMART
```

---

For example, the package [`stopwords`](https://github.com/quanteda/stopwords) provides a multilingual list of stop words that can be used to filter from (see )


```r
# Explore the first 20 stop words in Chinese.
head(stopwords::stopwords(language = "zh", source = "misc"), 20)
```

```
##  [1] "按"     "按照"   "俺"     "们"     "阿"     "别"     "别人"   "别处"  
##  [9] "是"     "别的"   "别管"   "说"     "不"     "不仅"   "不但"   "不光"  
## [17] "不单"   "不只"   "不外乎" "不如"
```

```r
# Count the number of total stop words for a given language.
length(stopwords::stopwords(language = "zh", source = "misc"))
```

```
## [1] 647
```

```r
# Stopword returns a vector of words. We could store those results in a variable
# and reuse it later.
```

---

Once we have a list of stop words, we need to remove them from our corpus. We can do that in several ways:


```r
raw_text &lt;- reddit_df[c("X", "comment")]
```


---

You may also remove words that are either important to some texts or approaches to the text (see Dobson, 2019:49-57). .highlight[This should be a step for reflection in the use of algorithms to create a reading]. What, if anything, has been removed or added and why? 

In our exercise, we use the Reddit data as it is. However, it can be useful to break down the texts into smaller chunks (Jockers, 2013; Buurma, 2015). Rather than process the text in one chuck, it can be broken down into a number of words, by chapter, by subreddit, or by paragraph. Breaking down the test like this will affect the topics that are read or how they are applied to the text and will give alternate readings. Another key question is what is a document? Is it the subreddit or a post or a comment? These choices will affect the type of language that we use and the returned results.

These are questions that you should be applying to data cleaning and preparation processes. Although they occur before the processing happens, these steps can alter what the reading. 

---

layout: false
class: interaction

## Lab Exercise

In your groups (10 min):
1. review the data that you have from week 2, and 
2. write down some questions and remarks about the format of the data and the processing that you have undertaken already. You can do this in shared document or using pen and paper, if easier.

Share and discuss findings with the class

---

## Modelling the Subreddits

In this exercise, we will use the subreddit data again. The subreddits are split up into documents with the name of the subreddit and the document id. We can use this later.  They are then split up into word frequencies and we remove the stop words. 



```r
# Create a new column `document`combining values from different columns. We will
# use it as a unique identifier (id)
by_subreddit &lt;- reddit_df %&gt;%
  unite(document, subreddit, X)

# now split the comment columns into words
by_comment_word &lt;- by_subreddit %&gt;%
  unnest_tokens(word, comment)

#remove the stop words and generate the counts
word_counts &lt;- by_comment_word %&gt;%
  anti_join(stop_words) %&gt;%
  count(document, word, sort = TRUE) %&gt;%
  ungroup()
```

```
## Joining, by = "word"
```

```r
# TODO: explore what we just did!
```

???

Now that you are aware of the effects of data cleaning and preparation on the underlying data, we can now begin to process it. 

---

class: interaction

.pull-left[
Let's explore what we have just done:


```r
# Display the first 20 rows. I.e. the top-20 most frequent words.
head(word_counts, 20)
```

```
##              document              word  n
## 1   MillennialBets_93           lithium 22
## 2   MillennialBets_93          americas 17
## 3   MillennialBets_93              2022 14
## 4   MillennialBets_93             stock 10
## 5  wallstreetbets_360               lac  9
## 6  wallstreetbets_387           battery  9
## 7  climatedisalarm_54 environmentalists  8
## 8   MillennialBets_93             https  8
## 9   MillennialBets_93               jan  8
## 10       Scotland_218              jobs  8
## 11       Scotland_225               job  8
## 12 wallstreetbets_359               amp  8
## 13 wallstreetbets_359           message  8
## 14         climate_15         political  7
## 15         climate_15           support  7
## 16  MillennialBets_93               lac  7
## 17 wallstreetbets_387           lithium  7
## 18 wallstreetbets_401             water  7
## 19         climate_28           climate  6
## 20         climate_28        government  6
```

]

.pull-right[

**Results interpretation (and refinement)**

* What processing choices have been made in this code? 
* How can they be read using the results of the above exercise?
]

---

### Refining the results


```r
# Create a vector of words that we have detected and want to get rid of.
*unnecessary_words &lt;- c("https", "2022", "jan")

# We are repeating previous code in a more concise way + add an extra row.
word_counts &lt;- reddit_df %&gt;%
  unite(document, subreddit, X) %&gt;% 
  # Split the comment columns into words.
  unnest_tokens(word, comment) %&gt;% 
  # Remove the stop words
  anti_join(stop_words) %&gt;%
  # Remove manual words.
* filter(!word %in% unnecessary_words) %&gt;%
  # Generate counts.
  count(document, word, sort = TRUE) %&gt;%
  ungroup()
```

```
## Joining, by = "word"
```

```r
# TODO: explore the results! (next slide)
```

---

Again, let's explore the outcome:


```r
head(word_counts, 20)
```

```
##              document              word  n
## 1   MillennialBets_93           lithium 22
## 2   MillennialBets_93          americas 17
## 3   MillennialBets_93             stock 10
## 4  wallstreetbets_360               lac  9
## 5  wallstreetbets_387           battery  9
## 6  climatedisalarm_54 environmentalists  8
## 7        Scotland_218              jobs  8
## 8        Scotland_225               job  8
## 9  wallstreetbets_359               amp  8
## 10 wallstreetbets_359           message  8
## 11         climate_15         political  7
## 12         climate_15           support  7
## 13  MillennialBets_93               lac  7
## 14 wallstreetbets_387           lithium  7
## 15 wallstreetbets_401             water  7
## 16         climate_28           climate  6
## 17         climate_28        government  6
## 18         climate_33                gt  6
## 19         climate_47               law  6
## 20 climatedisalarm_54           warming  6
```



---

### Recap

.pull-left[
We get a document created by the subreddit and the document id (`document`) with the word (`word`) and how many times it appears (`n`). The comment text now exists as series of word frequencies  to allow the machine to re-format it. 

We should be aware of not only how the cleaning process is put into code but also how the machine re-formats the text into a computational form. 
]


.pull-right[

|document           |word              |  n|
|:------------------|:-----------------|--:|
|MillennialBets_93  |lithium           | 22|
|MillennialBets_93  |americas          | 17|
|MillennialBets_93  |stock             | 10|
|wallstreetbets_360 |lac               |  9|
|wallstreetbets_387 |battery           |  9|
|climatedisalarm_54 |environmentalists |  8|
|Scotland_218       |jobs              |  8|
|Scotland_225       |job               |  8|
|wallstreetbets_359 |amp               |  8|
|wallstreetbets_359 |message           |  8|
|climate_15         |political         |  7|
|climate_15         |support           |  7|
|MillennialBets_93  |lac               |  7|
]



---

## Document Term Matrix

The next step is to convert the frequencies into a .highlight[Document Term Matrix (DTM)] to allow the algorithm to run. A DTM is where:

.pull-left[
- Each .highlight[row] represents a .highlight[document], 
- Each .highlight[column] represents a .highlight[word], 
- Each .highlight[value] shows .highlight[occurrences] (i.e. how often that word appears in the document). 
]

.pull-right[
![Each row can be sparse, that is they will not contain every term and have a lot of 0s in them. ](https://miro.medium.com/max/1400/1*XkR7ANXlWRqJ5TQvF4j91w.png)
_Each row can be sparse, that is they will not contain every term and have a lot of 0s in them._ ]


We use the `tidytext::cast_dtm` function to convert our word counts into a DTM. Matrices can be used to determine clusters of events and to carry out mathematical operations efficiently. 


```r
comments_dtm &lt;- word_counts %&gt;%
  cast_dtm(document, word, n)

# Explore what we just did:
comments_dtm
```

```
## &lt;&lt;DocumentTermMatrix (documents: 447, terms: 3737)&gt;&gt;
## Non-/sparse entries: 8614/1661825
## Sparsity           : 99%
## Maximal term length: 56
## Weighting          : term frequency (tf)
```

???

What does this suggest about the format that is being used? What has been selected?

---

## Creating a LDA model

Now that the document is in a matrix, we can apply .highlight[Latent Dirichlet Allocation (LDA)] to the words to identify the places where they might co-occur. Each topic is a dense cluster of associated words.  
We set up `n` clusters that we want to find to create a model with a given number topics. This is an arbitrary number based on the amount of subreddits. I have used a fixed seed (`seed=1234`) to provide some repeatability for the lab.


```r
number_clusters = 5 # Arbitrary number based on subreddits.
comments_lda &lt;- LDA(comments_dtm, k = number_clusters, control = list(seed = 1234))
comments_lda
```

```
## A LDA_VEM topic model with 5 topics.
```

We now have an LDA model of the texts that we put in. The data was re-formatted and cleaned before being converted through various forms to allow it to be made into clusters that we call topics.  

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
This is the core of topic modelling: the creation of the LDA model. The following sections will refer to this model as we begin to untangle it. ]

---

class: interaction

### Experiment

In your own time, look at the previous code again:


```r
number_clusters = 5 # Arbitrary number based on subreddits.
comments_lda &lt;- LDA(comments_dtm, k = number_clusters, control = list(seed = 1234))
comments_lda
```

Try to change the following values and observe what effects do they produce:

* Remove the `seed=1234` and see what happens. Try to change it with a different number, too.
* What happens when we change the number of clusters? 

You may want to read the function's documentation:

```R
?LDA

```

---

layout: true

## Reading the Topics

---

The next step is engage with the topics by both reading and visualising them. The algorithmic readings aid us in reducing text to a series of data points, topics, that suggest potential readings. We should be wary of such readings and approach them critically, such as the question of the social (Buurma, 2015).

The first thing that we should do is to investigate the topics to see what they are. The model will structure the word clusters numerically. The main task is to look at the words in each cluster and to identify the theme that links them or what they might represent. It might help you to assign a name to this so that you can describe it (Jockers, 2013). 

There is a limitation where you will need to think about what these terms represent and to place them into a wider social context. What term do the words imply and does it have the same connotation as the word that we might use? Rachel Buurma discusses the word social (Buurma, 2015) and Elisabeth Calloway _et al_ discuss how topics can have implications (Calloway et al, 2020) for the reading. It is more than a case of reading the topics, but reflecting on them. 

---


```r
# Let's explore our LDA again:
comments_lda
```

```
## A LDA_VEM topic model with 5 topics.
```

Regrefully, a LDA object does not provide much information when calling it by its name, as it is not a dataframe, like most of the data we've been using so far. 

---

The [`tidytext::tidy()`](https://juliasilge.github.io/tidytext/reference/stm_tidiers.html) function, taken from the tidytext package, .highlight[converts the Document Term Matrix into a dataframe]. The beta option for matrix shows the data as per-term-per-topic. So we will get terms for topics. 



```r
# Convert a LDA object into a dataframe.
comment_topics &lt;- tidy(comments_lda, matrix = "beta")

# Explore the result.
comment_topics
```

```
## # A tibble: 18,685 × 3
##    topic term          beta
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1     1 lithium  2.69e-214
##  2     2 lithium  1.07e-  2
##  3     3 lithium  6.52e- 11
##  4     4 lithium  5.90e-  3
##  5     5 lithium  2.36e-  2
##  6     1 americas 8.75e-221
##  7     2 americas 8.12e-  3
##  8     3 americas 1.34e-219
##  9     4 americas 4.69e-  4
## 10     5 americas 3.90e-  3
## # … with 18,675 more rows
```

---

We can provide visualisations of the topics that are generated by the model. 


```r
# Create top 5 words by topic.
top_terms &lt;- comment_topics %&gt;%
  group_by(topic) %&gt;%
  top_n(5, beta) %&gt;%
  # ungroup() %&gt;%
  arrange(topic, -beta)

top_terms
```

```
## # A tibble: 25 × 3
## # Groups:   topic [5]
##    topic term       beta
##    &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;
##  1     1 minister 0.0141
##  2     1 day      0.0140
##  3     1 gt       0.0111
##  4     1 climate  0.0109
##  5     1 days     0.0106
##  6     2 job      0.0228
##  7     2 people   0.0166
##  8     2 day      0.0131
##  9     2 hours    0.0131
## 10     2 days     0.0126
## # … with 15 more rows
```

---

.pull-left[

We can use `ggplot` to create a facetted barplot per topic:


```r
top_terms %&gt;%
  mutate(term = reorder_within(term, beta, topic)) %&gt;%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() 
```
]

.pull-right[

![](week7-slides_files/figure-html/visualiseTopicsModels2-1.png)&lt;!-- --&gt;
]

---

One method of using topic models is to assign thematic names to them so that they can be referenced. It also helps the reader of any report to understand the meaning that you assign. What does the language suggest? How does our original data selection affect the results? (Hint: what was the original query?).

One challenge that readers of topics face is that of synonyms. The algorithms look at complex language and reduce it to a series of probabilities, but this means the synonyms or subtle language forms can be ignored within the reading (Ben-David &amp; Soffer, 2019). This linguistic challenge is not limited to topic model but is present in other forms of computational reading, such as sentiment analysis. It is one that you might want to reflect on in your readings. 

Another method is to look at the types of words and language in the topics. Are they emotional words? Do they reference another theme as a response to it? Are the words dialect or jargon? A group read the Gamestop boards last year and a theme was the financial language that used in that area.

If you split your data to look at differing perspectives to an event, then the topics should be compared to discover whether they are asymmetrical (Beytia and Wagner, 2020) or if they show different responses and perspectives from the thread that is being commented upon (Ben-David &amp; Soffer, 2019). 

When you read the topics, it might be of interest to try a different number of clusters in the model above to see how this changes the readings. 


---

layout: false

### Applying Topics to Subreddits

We will now apply the LDA model to the comments and classify the topics by comment so that we can see how they are shared across the subrreddits. We use gamma option in matrix to identify the topic per document probabilities. Keep in mind that the topics are statistical probabilities rather than a human constructed topic. 


```r
# Convert to data frame.
comments_gamma &lt;- tidy(comments_lda, 
*                      matrix = "gamma")
# Separate it
comments_gamma &lt;- comments_gamma %&gt;%
  separate(document, c("subreddit", "id"), sep = "_", convert = TRUE)

comments_gamma
```

```
## # A tibble: 2,235 × 4
##    subreddit          id topic     gamma
##    &lt;chr&gt;           &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;
##  1 MillennialBets     93     1 0.000118 
##  2 wallstreetbets    360     1 0.000235 
##  3 wallstreetbets    387     1 0.000125 
##  4 climatedisalarm    54     1 0.0000996
##  5 Scotland          218     1 0.998    
##  6 Scotland          225     1 0.000204 
##  7 wallstreetbets    359     1 0.999    
##  8 climate            15     1 0.000202 
##  9 wallstreetbets    401     1 0.000283 
## 10 climate            28     1 0.999    
## # … with 2,225 more rows
```

---

Let's create a quick boxplot to visualise the subreddits by topic and to understand what topics might be in the subreddit. Boxplots are useful for viewing the distribution of numerical data and how that data is distributed. It is a quick way of showing how the topics are spread across the subreddits but they do take time to read. 

.pull-left[

```r
comments_gamma %&gt;%
  mutate(title = reorder(subreddit, gamma * topic)) %&gt;%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title) +
  labs(x = "topic", y = expression(gamma))
```
]

.pull-right[
![](week7-slides_files/figure-html/boxPlot2-1.png)&lt;!-- --&gt;
]

---

How can this help us interpret the topics that might be found within the subreddits? What sort of topics appear? (Hint: compare this graph with the previous one.) 

If we compare this with the previous graphs of the topics, can we identify the potential language or themes used and what does that suggest about the subreddit? These questions should encourage you to review your data and to identify examples of the pattern. As with the communities work in week 4, these patterns are algorithmic calculations to suggest patterns that we might not be aware of, but they should be treated with caution. 

---

class: interaction

## Lab Exercise

In your groups, take some time to investigate the topics that you have generated. A first pass might look at the construction of the topics from the above section. Take some time to think about what is in them. Are there terms that seem strange and what might it suggest about the cleaning processes? When you look at the terms, are there cases where the semantic meaning of a word might be different? How does topic modelling cope with synonyms?

In the second part, review how these appear within the subreddits. What type of topic do you see in them? Given what you know about the topic and the subreddit name, does it match what you would expect? Are new questions raised by the results that you have? Going back to week 1 and Elisabeth Calloway et al's discussion of topic modelling (Calloway et al., 2020), we should also be trying to contextualise the kind of social and medium questions come from our work. 

.b--light-yellow.ba.bw2.br3.shadow-5.ph4.mt5[
You will not be able to cover all of this in the time but these questions will help you get started.  
]

I will give you 20 minutes in your groups to do this and then we will discuss. 

---

## Critically reviewing the topics

The plots provide a way of viewing how the topics are assigned to the subreddits and we can use this to be critical about the topic modelling process. It is a glimpse of how the algorithms work that gives us a view of how the data is classified. We can already view this in the boxplots, but examining the parts of the structure allow us to gain a clearer view. Before we can visualise the data, we need to run a few conversion to reformat it and filter it correctly. 

Let us turn to the classifications within the topics. We will take the comments_gamma data frame and group it by subreddit, then the id. 


```r
comment_classifications &lt;- comments_gamma %&gt;%
  group_by(subreddit, id) %&gt;%
  slice_max(gamma) %&gt;%
  ungroup()
```

---


```r
comment_classifications
```

```
## # A tibble: 447 × 4
##    subreddit               id topic gamma
##    &lt;chr&gt;                &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
##  1 ClassPoliticsTwitter     1     5 0.996
##  2 climate                  2     4 0.812
##  3 climate                  3     4 0.983
##  4 climate                  4     1 0.998
##  5 climate                  5     4 0.998
##  6 climate                  6     4 0.995
##  7 climate                  7     4 0.997
##  8 climate                  8     4 0.695
##  9 climate                  9     4 0.993
## 10 climate                 10     4 0.957
## # … with 437 more rows
```

---

We can look at the places where the subreddit that the topic is associated with does not match the subreddit. In looking at this, we can critically think about the way that the topic model is guessing which topic to which a piece of text belongs. 



```r
# Take the classifications, group by subreddit and topic. 
subreddit_topics &lt;- comment_classifications %&gt;%
  count(subreddit, topic) %&gt;%
  group_by(subreddit) %&gt;%
  top_n(1, n) %&gt;%
  ungroup() %&gt;%
  rename(consensus = subreddit) %&gt;% 
  select(consensus, topic)


# Let's filter the subreddit classifications where it does not match consensus
# We'll join the subreddit and classifications together and filter where
# the subreddit and predicted subreddit do not match. 
comment_classifications %&gt;%
  inner_join(subreddit_topics, by = "topic") %&gt;%
  filter(subreddit != consensus)
```

```
## # A tibble: 1,646 × 5
##    subreddit               id topic gamma consensus          
##    &lt;chr&gt;                &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;              
##  1 ClassPoliticsTwitter     1     5 0.996 demsocialists      
##  2 ClassPoliticsTwitter     1     5 0.996 environment        
##  3 ClassPoliticsTwitter     1     5 0.996 PLUGgreenhydrogen  
##  4 ClassPoliticsTwitter     1     5 0.996 plugpowerstock     
##  5 ClassPoliticsTwitter     1     5 0.996 wallstreetbets     
##  6 climate                  2     4 0.812 climatepolicy      
##  7 climate                  2     4 0.812 collapse           
##  8 climate                  2     4 0.812 LateStageCapitalism
##  9 climate                  2     4 0.812 PLUGgreenhydrogen  
## 10 climate                  2     4 0.812 thatHappened       
## # … with 1,636 more rows
```

---

The matrix here can be augmented, using augment to add the columns from the LDA analysis to the Document Term Matrix to get a better view on how the data is created. We separate the names that we originally created to identify the subreddits and comment ids. 


```r
assignments &lt;- augment(comments_lda, data = comments_dtm)

assignments &lt;- assignments %&gt;%
  separate(document, c("subreddit", "id"), 
           sep = "_", convert = TRUE) %&gt;%
  inner_join(subreddit_topics, by = c(".topic" = "topic"))
head(assignments)
```

```
## # A tibble: 6 × 6
##   subreddit         id term    count .topic consensus     
##   &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         
## 1 MillennialBets    93 lithium    22      2 LabourUK      
## 2 MillennialBets    93 lithium    22      2 MillennialBets
## 3 MillennialBets    93 lithium    22      2 Scotland      
## 4 wallstreetbets   360 lithium     4      4 climate       
## 5 wallstreetbets   360 lithium     4      4 climatepolicy 
## 6 wallstreetbets   360 lithium     4      4 collapse
```


---

The top of the assignments is shown to help you understand what the data is. You can see the two topic assignments for the same word, which is the assignment by the topic model. It is not the easiest form to read, so we will visualise it. 

If you want to explore particualr topic further, a simple tool has been written below for you. We can filter the data by a topic to identify the words and what they might come from. You can alter id == ??? to term==??? to identify terms, if you want to filter it further. 



```r
docs &lt;- assignments %&gt;%
  filter(id==262) %&gt;%
  arrange(desc(count))
docs
```

```
## # A tibble: 215 × 6
##    subreddit    id term  count .topic consensus        
##    &lt;chr&gt;     &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;            
##  1 Scotland    262 gt        3      3 climatedisalarm  
##  2 Scotland    262 gt        3      3 electricvehicles 
##  3 Scotland    262 gt        3      3 nextfuckinglevel 
##  4 Scotland    262 gt        3      3 PLUGgreenhydrogen
##  5 Scotland    262 gt        3      3 thatHappened     
##  6 Scotland    262 life      3      3 climatedisalarm  
##  7 Scotland    262 life      3      3 electricvehicles 
##  8 Scotland    262 life      3      3 nextfuckinglevel 
##  9 Scotland    262 life      3      3 PLUGgreenhydrogen
## 10 Scotland    262 life      3      3 thatHappened     
## # … with 205 more rows
```

---

We can visualise the way that the topics are constructed by creating a graph of the way that the topics are matched. This allows us to see how the machine is viewing the language that is being used. 


.panelset[
.panel[.panel-name[R Code]

```r
library(scales)

assignments %&gt;%
  count(subreddit, consensus, wt = count) %&gt;%
  mutate(across(c(subreddit, consensus), ~str_wrap(., 20))) %&gt;%
  group_by(subreddit) %&gt;%
  mutate(percent = n / sum(n)) %&gt;%
  ggplot(aes(consensus, subreddit, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "darkred", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Comment words were assigned to",
       y = "Comment words came from",
       fill = "% of assignments")
```
]
.panel[.panel-name[Plot]
![](week7-slides_files/figure-html/visualiseTopics-1.png)
]

]



How does this graph help interpret how the process is interpreting the data? How does it help identify places where language might be shared?

---

class: slide-primary

## Conclusion

In this lab, we build on the previous text analysis to identify the ways that machines might help us read and understand a text. TF-IDF uses ranking to identify the closeness of the documents and to determine their similarity. In contrast, topic modelling takes a statistical approach to the discovering statistical densities that it calls topics and groups the documents together in that way. As well as being used to explore the data by language, we can use the critical thinking from the previous sessions to think about how the methods presented here work, such as considering ranking or the use of probabilities. 

The questions that we should be raising concern the choices of methods and their affordances; what criteria are used to select, reduce and format the data; how does the method operate - that is, what does it use to create its result? 

The approaches presented here and in the previous two session focus on the documents presented to them, so they are platform agnostic. However, the platform limitations may come into play here, such as constraints on the language length or the type of language that might be used. Unlike the search engine week where we re-purposed the device (the search engine), these methods build on the device (the API) and incorporate it. In each week, we have altered the format of the data to the technique being used. In APIs, we convert JSON into CSVs or data frames. These are transformed into igraph objects in week 4 and then Gephi objects. In the last two sessions, we have transformed a text column into numeric values. When we work with digital methods, we need to be aware of these issues.  

---

layout: true

## Bibliography

---

Ben-David, Anat, and Oren Soffer. 'User Comments across Platforms and Journalistic Genres'. Information, Communication &amp; Society 22, no. 12 (15 October 2019): 1810–29. 

Dobson, J. 2019. "Can an Algorithm be Disturbed? Machine Learning, Intrinsic Criticism, and the Digital Humanities" in Critical Digital Humanities: The Search for a Methodology, University of Illinois Press, Urbana, Chicago and Springfield

Rieder, B. 2020. “From Frequencies to Vectors” in Engines of Order: A Mechanology of Algorithmic Techniques. Amsterdam University Press, Amsterdam.

Rieder, B. 2020. “Interested Learning” in Engines of Order: A Mechanology of Algorithmic Techniques. Amsterdam University Press, Amsterdam.

Silge, J and Robinson, D. 2013. Text Mining with R. O'Reilly, Sebastopol.

---

### Topic Modelling

Blei, D.M., Ng, A.Y. and Jordan, M.I., 2003. Latent Dirichlet Allocation. The Journal of Machine Learning research, 3, pp.993-1022. &lt;https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf&gt;

Buurma, R.S., 2015 The fictionality of topic modeling: Machine reading Anthony Trollope’s Barsetshire series. Big Data &amp; Society 2:2.

Callaway, E, Turner, J, Stone, H, Halstrom, A. 2020.The Push and Pull of Digital Humanities: Topic Modeling the “What is digital humanities?” Genre. Digital Humanities Quarterly. 14: 1. 

Topic Modeling, Computing for the Social Sciences, &lt;https://cfss.uchicago.edu/notes/topic-modeling/&gt;

Intuitive Guide to Latent Dirichlet Allocation, &lt;https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
