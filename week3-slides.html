<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 3 - Visual analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Iain Emsley, Carlos Cámara-Menoyo" />
    <script src="week3-slides_files/header-attrs-2.11/header-attrs.js"></script>
    <link href="week3-slides_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="week3-slides_files/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="week3-slides_files/tile-view-0.2.6/tile-view.js"></script>
    <script src="week3-slides_files/js-cookie-3.0.0/js.cookie.js"></script>
    <script src="week3-slides_files/peerjs-1.3.1/peerjs.min.js"></script>
    <script src="week3-slides_files/tiny.toast-1.0.0/toast.min.js"></script>
    <link href="week3-slides_files/xaringanExtra-broadcast-0.2.6/broadcast.css" rel="stylesheet" />
    <script src="week3-slides_files/xaringanExtra-broadcast-0.2.6/broadcast.js"></script>
    <script src="week3-slides_files/xaringanExtra-progressBar-0.0.1/progress-bar.js"></script>
    <link rel="stylesheet" href="week3-slides_files/sass75641530e21bdeef74341035d8072c12/warwick.min.css" type="text/css" />
    <link rel="stylesheet" href="week3-slides_files/sassc87f85860c3bf1f886619f4ace23e9bf/warwick-fonts.min.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Week 3 - Visual analysis
## IM904: Digital Objects, Digital Methods
### Iain Emsley, Carlos Cámara-Menoyo
### Centre for Interdisciplinary Methodologies

---


<style>.xe__progress-bar__container {
  bottom:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #552D62;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>


## Introduction

Visual media  - from images to gifs to videos - are increasingly prevalent online. Images are also being read by machines to extract features or other data to be presented as different readings or collections - such as search engines. 

Images can be shared as networks using hashtags or comments by users or ranked by platforms such as search engines. As noted by Sabine Niederer and Gabrielle Colombo (Niederer and Colombo, 2019: 44), platforms and users do not order or describe images in the same way.

In the previous labs, we have looked at the getting various types of data such as search results, text from APIs and images. Having found data, the labs will now turn to forms of analysis.  This week, we turn to images. Like the first week, we are going to use some existing methods from a seminar reading. We will be looking at Google Images and thinking about how to analyse them. While we cannot cover the whole field, we will provide some techniques to get you started, building on the image search exercise from the first week and images that you may come across online. The focus is on qualitative methods that develop our use of image search in week 1. 

---

A browser plugin that may be of use is DownThemAll:

https://www.downthemall.org

It might help if you have many images that you want to collect. 

---

## Learning Objectives

After this lesson, you should be able to:

*	Conduct forms of visual analysis;
*	Understand the affordances and format issues involved;


---

## Networked Images

Social media relies on various types of networks to link information and people together. We will look at the hashtags and mentions as well as URLs that are linked next week. Who posts the tweet and at what time? These are not the only network that we might want to investigate.

This sort of analysis treats images as data. We are not thinking about its content but its metadata. Phototrails (Hochman and Manovich, 2013; Manovich, Hochman, and Chow, 2013) uses a tool to organise images from particular locations to discover patterns. Lev Manovich has also created visualisations from image collections (Manovich, 2020: 176-180) to represent them in other ways, such as creating arranging the images by date in a histogram. While we are not using such software, these techniques can be partially repurposed to interrogate them. 

---

class: interaction

### Exercise

.pull-left[
In this first exercise, we will look at the networks that can be found within networks using a close reading. Social media posts may contain images that we can further read to get other information from them.

We might now only follow the hashtags or mentions to discover other images? We should also look at how the hashtag is used and whether it is linked to a person or a phrase?

* What sort of data can we find in this image? 
* How does it situate the image within the network? 
* If users have liked or retweeted, do they share other images? Are images linked through by the hashtags? What sort of images are they and are they altered? 

Following the links allows us to create both a collection of images as well as the network that supports their discovery.

]

.pull-right[

![](figs/visual/week3-networkedimage.png)

]

---

### Lab Exercise

In your groups, can you find an image on Twitter and apply these questions to it?

You will get 10 minutes to find and discuss the image before we share our findings in a group discussion.

---

## Collections of Images


Two approaches to working with images:

1. **Images as data**: colours, brightness...
2. **Images as content**: we care about what the image has in it: _what is it portraying?_

???


Having understood how we can conduct close readings of images, we can begin to think about how we do the same to collections. Here we turn to .highlight[using **images as content**], so we are thinking of the what the image represents. In doing this, we are mixing methods and making choices about these methods. The abundance of images provide multiple potential methods and it is important that we consider the affordances and what it is that we are producing (Pink, 2017).

There are two main approaches to this: firstly, we can look at these as data or, secondly, we can look at these as content. By looking at images as data, we are looking at colours and brightness instead of the content. By looking at images as content, we care about what the image has in it: what is it portraying?

When we study visual data, the output is often a visual output, such as a graph form or a new image. Gabrielle Colombo mentions using  to create a new composite image. Various types are outlined:

*  Grid - where the images are put into a grid format with the most important at the top. Here one needs to define the value of most important, such as most likes or the most common. 

*  Time - where the images are laid out in a chronological order to tell a story. 

*  Cluster - where images are ordered by a particular theme, such as a location or feature. These might be enhanced by giving them a spatial aspect - are types of features closer more alike than others? 

The structure that you use will create a focus for the viewer's understanding of the choices. Your choice of images will also affect your reader's interpretation. Why are you choosing certain images - is it their engagement (likes or shares), their diversity, or their similarity? Do they represent a particular platform? 

---

class: interaction

### Recipe

This recipe is derived from the Visual Methods paper (Niederer and Colombo, 2019) which is worth reading in full.

1. Take a query term, such as "climate change".

2. Enter it into a search engine's image search. 

3. Define the values that you are going to use. Make a note of these values. 

4. Define the composite image to be built. Are you using a grid, timeline, or a cluster? You may use another form entirely. 

5. Download the images that are required. 

6. Create image and write notes about the choices made. Why are you clustering them in this way? What features are you looking for? What might it be suggesting?

You should make notes on the choices made and paths taken either in a separate and shared, document or using notes on a notepad. 

One example that was created using Google Images search and Warwick's access to Padlet is here:
https://warwick.padlet.org/iainemsley/xrv64il4qdztwlmc

It is an example that uses the Canvas template in Padlet to cluster some images found using Google's Image Search with some notes.

---

### Lab Exercise

In your groups again, choose a theme and then create your own cluster image that will be discussed by the group. 

Follow the recipe given with your own query term.

If you are developing your own metrics, then please do read Niederer and Colombo (2019:57) where they discuss how they came with a cross-platform value for likes of an image. This is part of platform affordances and the challenges of working across platforms. 

Another method might be to look at what appears and disappears over time, if you are ordering by it (Pearce and De Gaetano, 2021). Do trends or patterns appear from this? 

---

#### Further Notes
If you are using images taken from a search engine, make a note of the domains that used that image and the time. You may also want to note the image title. Store these in a spreadsheet with the image name. 

This data can be used in different ways. 

When we use Gephi, we can create a mention network of the images and their domains as a network. 

You might also make notes about the features that you find when you organise your images against the image names so that you can visualise them in a different way. 


---

class: slide-primary

## Conclusion

In this lab, we have used images to create a simple analysis using visual images. By starting with a close reading, we can identify forms of the analysis. We should also think about how this can be linked to other forms of analysis, such as networks or text, to enrich our work. As noted, visual methods are a suggestion towards further research (Niederer and Colombo, 2019) and can be enhanced through reading the metadata, such as domain or hashtags. The use of images can also be used to explore the limits of the collections that are given to us (Pearce and De Gaetano, 2021), developing the approaches from week 1. 

In the second half of this lab, we will be discussing the first part of the group assignment. 

---

## Bibliography

Colombo, G., Niederer, S., 2021. Diseña 19 | Visual Methods for Online Images: Collection, Circulation, and Machine Co-Creation 7.

Hochman, N., Manovich, L. Zooming into an Instagram City: Reading the local through social media, First Monday, Volume 18, Number 7 - 1 July 2013, https://firstmonday.org/ojs/index.php/fm/article/download/4711/3698, doi:10.5210/fm.v18i7.4711

Manovich, L. Hochman, N. and Chow, J. 2013. Phototrails. http://phototrails.info

Manovich, L. 2020. Cultural Analytics. MIT Press, Cambridge, Mass. 

Niederer, S. (2018). Networked images: visual methodologies for the digital age. Hogeschool van Amsterdam.
https://pure.hva.nl/ws/files/4959407/networked_images_1_.pdf

Niederer, S., Colombo, G., 2019. Visual methodologies for networked images: Designing visualizations for collaborative research, cross-platform analysis, and public participation. Disena 14, 40–67. https://doi.org/10.7764/disena.14.40-67

Pearce, W., &amp; De Gaetano, C. (2021). Google Images, Climate Change, and the Disappearance of Humans. Diseña, (19), Article.3. https://doi.org/10.7764/disena.19.Article.3

Pink, S. 2017. "Technologies, Possibilities, Emergence and an Ethics of Responsibility: Refiguring Techniques" in Gómez Cruz E., Sumartojo, S., Pink, S. (eds). Refiguring Techniques in Digital Visual Research. Palgrave, London. 


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
