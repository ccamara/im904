---
title: "Week 7 - Text analysis (2)"
subtitle: "IM904: Digital Objects, Digital Methods"
author: "Iain  Emsley, Carlos Cámara-Menoyo"
institute: "Centre for Interdisciplinary Methodologies"
# date: "08/12/2020"
output:
  xaringan::moon_reader:
    # css: ["default", "extra.css"]
    css: ["css/warwick.scss", "css/warwick-fonts.scss", "default", "widths.css"]
    seal: true #Set false to remove default title slide.
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      # titleSlideClass: ["right", "top", "my-title"]
    yolo: false
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)

# Xaringan Extra configuration. +info: https://pkg.garrickadenbuie.com/xaringanExtra
xaringanExtra::use_tile_view()
xaringanExtra::use_broadcast()
# xaringanExtra::use_scribble()
xaringanExtra::use_clipboard()
xaringanExtra::use_fit_screen() # Press ALT/Option+F
xaringanExtra::use_tachyons()
xaringanExtra::use_panelset()
xaringanExtra::use_progress_bar(color = "#552D62", location = "bottom")

```

## Previously...

Here we build on the last text analysis lab. You have used text analysis to identify patterns in our data  using techniques such as ngrams, and bigrams in particular, and  to explore language through frequently occurring terms and used visualisation as an  exploratory method. 

---

## Text analysis 2: Topic Modelling

In this, we turn to purely .highlight[algorithmic methods] that are becoming more common. We will use .highlight[**Topic Modelling**: an unsupervised machine learning to automate our reading of the data]. Instead of guiding the model by tagging data, we let the algorithm identify what we should attend to by identifying points of density between words (as clusters) and then between densities of documents that share these clusters. When we read a topic model, we are not reading the texts but the statistical model of the texts. That means that an algorithm focuses on what language to use and discard and, unlike the previous session, you may not know what data has been ignored. You may wonder if this is a sensible idea and that is an excellent opportunity to reflect on algorithmic criticism. 

Here we begin to use Topic Modelling to take high level approaches to our data and to think about the patterns that might be found in it. Like the first text analysis session and networks, we cannot hope to cover the topics in full in this lab but it should provide enough information to reflect on these approaches. I hope that we can have a group discussion at the end to reflect on the issues raised in this lab and what it means for digital methods presented here and in the previous sessions. 

In this lab, we are going to use topic modelling to reflect on two papers, Thelwall's _Social Media for Youtube_ (2018) and Beytia and Wagner's _Visibility Layers_ (2020). We explore the idea of .highlight[topical asymmetry] from Beytia and Wagner through topic modelling and by using the critical questions as to how to the corpus is created using James Dobson's critique of topic modelling (2019).

---

## Learning Objectives

By the end of this lesson, you will have a basic understanding of:

* topic modelling;
* critical approaches to topic modelling;
* the differences and similarities of algorithmic versus computational reading. 

---

layout: true
## Requirements

---
Before we get started, we need to set our environment up. We will need some new packages to handle the topic modelling on top of the ones loaded for the last session. 

```{r install, eval=F}
install.packages('dplyr')
install.packages('tidytext')
install.packages('forcats')
install.packages('topicmodels')
install.packages('reshape2')
```

.bg-washed-red.b--dark-red.ba.bw2.br3.shadow-5.ph4.mt5[
**Side Note for Homebrew users**:  
If you have installed R using Homebrew, you will need to alter a file to complete the installation of the topicmodels package. The details are on this page:
[https://tinyheero.github.io/2016/02/20/install-r-topicmodels.html](https://tinyheero.github.io/2016/02/20/install-r-topicmodels.html)
]

---


If you have installed the packages, you will not need to do this again. You will need to tell R to use them though. 

```{r libraries, message=FALSE, warning=FALSE}
library(dplyr)
library(tidytext)
library(ggplot2)
library(forcats)
library(topicmodels)
library(stringr)
library(tidyr)
library(reshape2)
```

---

We will use our Reddit data again. If you do not have any Reddit data, you can use this file:
<https://files.warwick.ac.uk/iainemsley/files/IM904/reddit_parisagreementclimate_2021-01-27+094516_.csv>
Please change this path to your data.
```{r data}
# Load the data.
dataFile = "data/week2/reddit_COP26_2022-01-18.csv"
reddit_df <- read.csv(dataFile, stringsAsFactors = FALSE, encoding = 'UTF-8')

# Uncomment below to explore the data to see if it's what we wanted to load.
# head(reddit_df)
# View(reddit_df)
```

In this lab, we will be using .highlight[**topic modelling** as the main technique]. We will reflect on the Ben-David et al (2019) paper and use the same algorithm to explore the Reddit data.  

---

layout: false

## Topic Modelling

.pull-left[
We can use topic modelling to identify potential areas and threads of discussion. Topic Models break up the document into a series of categories based on probability. The [Latent Dirichlet Allocation (LDA) algorithm](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (Blei, Ng, and Jordan, 2003) is used to identify potential topics based on word similarity. Topic Modelling applies these topics to the documents and provides a statistical count for the likelihood that this topic is present. ]


.pull-right[
![](https://thatware.co/wp-content/uploads/2020/07/LDA-topic-modeing-1024x515-1.png)
]

---

## What are we processing?


In the first step, **we need to think carefully about what we are modelling**. Here we should think about 

* the .highlight[format of the data that we are using] - is it Reddit, Twitter, Weibo or similar? 
* whether, and how, we might need to clean up our data. -> This depends on the format of the data (remember affordances?)
  * If we use Reddit comments, then we might want to think about how the documents are divided. In the exercise, we will use the Reddit data and you should be attentive to how it is structured: it is subdivided by subreddit (if at all), by post and comment, and by author. 
  * Further processing may have taken place to identify particular posts and the data is grouped by these - think about the questions that Beytia and Wagner raise and what steps might be taken to approach them. Hint: think of what they are asking in the reports in their methods and work it backwards.

---

class: slide-secondary
## Recipe overview

1.  **Split the text into words (Tokenisation):** `tidytext::unnest_tokens()` breaks a column into independent words + transformations (to lower case and removes puntuation signs followed by space).

2. **Filter unneeded words (stopwords)**: `tidytext::stop_words` provides a list of frequent stop words in English. For multilingual texts we may want to use `stopwords` package

3.  **Count word frequencies:** `dplyr::count()` can group by variables and count number of ocurrences

4.  **Getting top words :** `dplyr::row_number()` or `head()`

5.  (optional) Filter unneeded words and repeat steps 1-4 until happy with results

6.  **Cast terms into a document-term matrix (DTM):** `tidytext::cast_dtm()` transforms a table in tidy format into a DTM

7. **Creating a LDA model:** `topicmodels::LDA()`

8. Exploring and interpreting the model

    1. **From LDA model to tidy table**: `tidytext::tidy()` converts a LDA object (model) into a tidy data frame.
    2. Exploring data via tables or Plots `ggplot()`
    
    
---

## Tokenisation

.panelset.sideways[
.panel[.panel-name[R Code]
```{r tokenisation, fig.show='hide'}
# Create a new column `document`combining values from different columns. We will
# use it as a unique identifier (id)
by_subreddit <- reddit_df %>%
  unite(document, subreddit, X) %>% 
  select(document, comment)

# now split the comment columns into words
by_comment_word <- by_subreddit %>% #<<
  unnest_tokens(word, comment)      #<<

# Explore the data (use use knittr:kable to see it in a nice table)
# knitr::kable(head(by_comment_word))
```
]
.panel[.panel-name[Output]
```{r}
# Explore the data (use use knittr:kable to see it in a nice table)
knitr::kable(head(by_comment_word, 10))
```

]

]


---

layout: true
## Data preparation

---

.pull-left[
A process that we might undertake is to prepare our data and one step is to **remove stop words**. Stop words are commonly occurring terms like `the`, `a`, `and`, `from` and so on. They are used frequently but may not add meaning to the data. Removing stop words can be tricky. 

R's `{tidytext}` has a list of `r nrow(tidytext::stop_words)` stop words that can be used and other tools have their own lists of words (i.e. [`{stopwords}`](https://github.com/quanteda/stopwords)). 

```{r}
nrow(tidytext::stop_words)
```


]

.pull-right[
```{r stop_words}
# Tidytext provides a data frame with 1149 English Stop Words
head(tidytext::stop_words, 15)

```
]




---

For example, the package [`{stopwords}`](https://github.com/quanteda/stopwords) provides a multilingual list of stop words that can be used to filter from (see week 5's content)

```{r stop_words_multi_lingual }
# Explore the first 20 stop words in Chinese.
head(stopwords::stopwords(language = "zh", source = "misc"), 20)

# Count the number of total stop words for a given language.
length(stopwords::stopwords(language = "zh", source = "misc"))

# Stopword returns a vector of words. We could store those results in a variable
# and reuse it later.

```

---

You may also remove words that are either important to some texts or approaches to the text (see Dobson, 2019:49-57). .highlight[This should be a step for reflection in the use of algorithms to create a reading]. 

* _What, if anything, has been removed or added and why?_

---

Once we have a list of stop words, we need to remove them from our corpus. We can do that in several ways:

```{r}

raw_text <- reddit_df[c("X", "comment")]



```


---

In our exercise, we use the Reddit data as it is. However, **it can be useful to break down the texts into smaller chunks** (Jockers, 2013; Buurma, 2015). Rather than process the text in one chuck, it can be **broken down into a number of words, by chapter, by subreddit, or by paragraph**. Breaking down the test like this will affect the topics that are read or how they are applied to the text and will give alternate readings. Another key question is what is a document? Is it the subreddit or a post or a comment? These choices will affect the type of language that we use and the returned results.

These are questions that you should be applying to data cleaning and preparation processes. Although they occur before the processing happens, these steps can alter what the reading. 

---

layout: false


class: interaction

## Lab Exercise

In your groups (10 min):
1. review the data that you have from week 2, and 
2. write down some questions and remarks about the format of the data and the processing that you have undertaken already. You can do this in shared document or using pen and paper, if easier.

Share and discuss findings with the class

---



## Count word frequencies (by subreddit)

In this exercise, we will use the subreddit data again. The subreddits are split up into documents with the name of the subreddit and the document id. We can use this later.  They are then split up into word frequencies and we remove the stop words. 


```{r tidyR}
# Create a new column `document`combining values from different columns. We will
# use it as a unique identifier (id)
by_subreddit <- reddit_df %>%
  unite(document, subreddit, X)

# Now split the comment columns into words
by_comment_word <- by_subreddit %>%
  unnest_tokens(word, comment)

# Remove the stop words and generate the counts
word_counts <- by_comment_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

# TODO: explore what we just did!
```

???

Now that you are aware of the effects of data cleaning and preparation on the underlying data, we can now begin to process it. 

---

class: interaction

.pull-left[
Let's explore what we have just done:

```{r}

# Display the first 20 rows. I.e. the top-20 most frequent words.
head(word_counts, 20)
```

]

.pull-right[

**Results interpretation (and refinement)**

Do you remember how did we generate the `document` column?

* What processing choices have been made in this code? 
* How can they be read using the results of the above exercise?
]

---

### Refining the results

```{r refining_data}
# Create a vector of words that we have detected and want to get rid of.
unnecessary_words <- c("https", "2022", "jan") #<<

# We are repeating previous code in a more concise way + add an extra row.
word_counts <- reddit_df %>%
  unite(document, subreddit, X) %>% 
  # Split the comment columns into words.
  unnest_tokens(word, comment) %>% 
  # Remove the stop words
  anti_join(stop_words) %>%
  # Remove manual words.
  filter(!word %in% unnecessary_words) %>% #<<
  # Generate counts.
  count(document, word, sort = TRUE) %>%
  ungroup()

# TODO: explore the results! (next slide)

```

---

Again, let's explore the outcome:

```{r}
head(word_counts, 20)
```



---

### Recap

.pull-left[
We get a document created by the subreddit and the document id (`document`) with the word (`word`) and how many times it appears (`n`). The comment text now exists as series of word frequencies  to allow the machine to re-format it. 

We should be aware of not only how the cleaning process is put into code but also how the machine re-formats the text into a computational form. 
]


.pull-right[
```{r echo=FALSE}
knitr::kable(head(word_counts, 13))
```
]



---

## Document Term Matrix

The next step is to convert the frequencies into a .highlight[Document Term Matrix (DTM)] to allow the algorithm to run. A DTM is where:

.pull-left[
- Each .highlight[row] represents a .highlight[document], 
- Each .highlight[column] represents a .highlight[word], 
- Each .highlight[value] shows .highlight[occurrences] (i.e. how often that word appears in the document). 
]

.pull-right[
![Each row can be sparse, that is they will not contain every term and have a lot of 0s in them. ](https://miro.medium.com/max/1400/1*XkR7ANXlWRqJ5TQvF4j91w.png)
_Each row can be sparse, that is they will not contain every term and have a lot of 0s in them._ ]


We use the `tidytext::cast_dtm` function to convert our word counts into a DTM. Matrices can be used to determine clusters of events and to carry out mathematical operations efficiently. 

```{r, convertToMatrix}
comments_dtm <- word_counts %>%
  cast_dtm(document, word, n)

# Explore what we just did:
comments_dtm
```

???

What does this suggest about the format that is being used? What has been selected?

---

## Creating a LDA model

Now that the document is in a matrix, we can apply .highlight[Latent Dirichlet Allocation (LDA)] to the words to identify the places where they might co-occur. Each topic is a dense cluster of associated words.  
We set up `n` clusters that we want to find to create a model with a given number topics. This is an arbitrary number based on the amount of subreddits. I have used a fixed seed (`seed=1234`) to provide some repeatability for the lab.

```{r convertToLDA}

number_clusters = 5 # Arbitrary number based on subreddits.
comments_lda <- LDA(comments_dtm, k = number_clusters, control = list(seed = 1234))
comments_lda
```

We now have an LDA model of the texts that we put in. The data was re-formatted and cleaned before being converted through various forms to allow it to be made into clusters that we call topics.  

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
This is the core of topic modelling: the creation of the LDA model. The following sections will refer to this model as we begin to untangle it. ]

---

class: interaction

### Experiment

In your own time, look at the previous code again:

```{r convertToLDA2, eval=FALSE}

number_clusters = 5 # Arbitrary number based on subreddits.
comments_lda <- LDA(comments_dtm, k = number_clusters, control = list(seed = 1234))
comments_lda
```

Try to change the following values and observe what effects do they produce:

* Remove the `seed=1234` and see what happens. Try to change it with a different number, too.
* What happens when we change the number of clusters? 

You may want to read the function's documentation:

```R
?LDA

```

---

layout: true

## Reading the Topics

---

The next step is engage with the topics by both reading and visualising them. The algorithmic readings aid us in reducing text to a series of data points, topics, that suggest potential readings. We should be wary of such readings and approach them critically, such as the question of the social (Buurma, 2015).

The first thing that we should do is to investigate the topics to see what they are. The model will structure the word clusters numerically. The main task is to look at the words in each cluster and to identify the theme that links them or what they might represent. It might help you to assign a name to this so that you can describe it (Jockers, 2013). 

There is a limitation where you will need to think about what these terms represent and to place them into a wider social context. What term do the words imply and does it have the same connotation as the word that we might use? Rachel Buurma discusses the word social (Buurma, 2015) and Elisabeth Calloway _et al_ discuss how topics can have implications (Calloway et al, 2020) for the reading. It is more than a case of reading the topics, but reflecting on them. 

---

.panelset.sideways[
.panel[.panel-name[Explanation]

Regrefully, a LDA object does not provide much information when calling it by its name, as it is not a dataframe, like most of the data we've been using so far. 

```{r exploreLDA}
# Let's explore our LDA again:
comments_lda
```

That's because LDA objects like `comments_lda` contain nested matrices. Two of them are particularly relevant: _beta_ and _gamma:_

* **beta** contains probabilities of words in topics
* **gamma** contains probabilities of topics in document

we can explore them using `@beta` or `@gamma`

]
.panel[.panel-name[Code]
We can see its structure by running this command
```{r}
str(comments_lda)
```

]

]

---
layout:true

### Applying probabilities of words in topics

---

The [`tidytext::tidy()`](https://juliasilge.github.io/tidytext/reference/stm_tidiers.html) function  .highlight[converts the Document Term Matrix into a dataframe]. Pay attention to `matrix` parameter. 

Since we are looking for probabilities of words in topics, .highlight[we'll use the _beta_ option for matrix (shows the data as per-term-per-topic)]. So we will get terms for topics. 


```{r viewTopics}
# Convert a LDA object into a dataframe.
comment_topics <- tidy(comments_lda, matrix = "beta")

# Explore the result.
comment_topics

```

---

We can provide visualisations of the topics that are generated by the model. 

```{r}
# Create top 5 words by topic.
top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  # ungroup() %>%
  arrange(topic, -beta)

top_terms
```

---

.pull-left[

We can use `ggplot` to create a facetted barplot per topic:

```{r visualiseTopicsModels, fig.show='hide'}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() 
```
]

.pull-right[

![](`r knitr::fig_chunk("visualiseTopicsModels", ".png")`)

]

---

One method of using topic models is to assign thematic names to them so that they can be referenced. It also helps the reader of any report to understand the meaning that you assign. What does the language suggest? How does our original data selection affect the results? (Hint: what was the original query?).

One challenge that readers of topics face is that of synonyms. The algorithms look at complex language and reduce it to a series of probabilities, but this means the synonyms or subtle language forms can be ignored within the reading (Ben-David & Soffer, 2019). This linguistic challenge is not limited to topic model but is present in other forms of computational reading, such as sentiment analysis. It is one that you might want to reflect on in your readings. 

Another method is to look at the types of words and language in the topics. Are they emotional words? Do they reference another theme as a response to it? Are the words dialect or jargon? A group read the Gamestop boards last year and a theme was the financial language that used in that area.

If you split your data to look at differing perspectives to an event, then the topics should be compared to discover whether they are asymmetrical (Beytia and Wagner, 2020) or if they show different responses and perspectives from the thread that is being commented upon (Ben-David & Soffer, 2019). 

When you read the topics, it might be of interest to try a different number of clusters in the model above to see how this changes the readings. 


---

layout: true

### Applying Topics to Subreddits

---

We will now apply the LDA model to the comments and .highlight[classify the topics by comment] so that we can see how they are shared across the subrreddits. We use gamma option in matrix to identify the topic per document probabilities. Keep in mind that the topics are statistical probabilities rather than a human constructed topic. 

```{r, convertGamma}
# Convert to data frame.
comments_gamma <- tidy(comments_lda, 
                       matrix = "gamma") #<<
# Separate it
comments_gamma <- comments_gamma %>%
  separate(document, c("subreddit", "id"), sep = "_", convert = TRUE)

comments_gamma
```

---


.pull-left[

Let's create a quick boxplot to visualise the subreddits by topic and to understand what topics might be in every subreddit. 

Boxplots are useful for viewing the distribution of numerical data and how that data is distributed. It is a quick way of showing how the topics are spread across the subreddits but they do take time to read. 


```{r gammaBoxPlot, fig.show='hide'}
comments_gamma %>%
  # mutate(title = reorder(subreddit, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ subreddit) + # You can replace it by title column.
  labs(title = "Topics distribution by subreddit",
       x = "topic", y = expression(gamma))
```
<small>How can this help us interpret the topics that might be found within the subreddits? What sort of topics appear? (Hint: compare this graph with the previous one.) </small>

]

.pull-right[
![](`r knitr::fig_chunk("gammaBoxPlot", ".png")`)

]

---
layout:false
### Interpreting results

If we compare this with the previous graphs of the topics, can we identify the potential language or themes used and what does that suggest about the subreddit? These questions should encourage you to review your data and to identify examples of the pattern. As with the communities work in week 4, .highlight[these patterns are algorithmic calculations to suggest patterns that we might not be aware of, but they should be treated with caution]. 

.pull-left[
![](`r knitr::fig_chunk("visualiseTopicsModels", ".png")`)

]

.pull-right[
![](`r knitr::fig_chunk("gammaBoxPlot", ".png")`)

]



---

class: interaction

## Lab Exercise

In your groups, take some time to investigate the topics that you have generated. A first pass might look at the construction of the topics from the above section. Take some time to think about what is in them. Are there terms that seem strange and what might it suggest about the cleaning processes? When you look at the terms, are there cases where the semantic meaning of a word might be different? How does topic modelling cope with synonyms?

In the second part, review how these appear within the subreddits. What type of topic do you see in them? Given what you know about the topic and the subreddit name, does it match what you would expect? Are new questions raised by the results that you have? Going back to week 1 and Elisabeth Calloway et al's discussion of topic modelling (Calloway et al., 2020), we should also be trying to contextualise the kind of social and medium questions come from our work. 

.b--light-yellow.ba.bw2.br3.shadow-5.ph4.mt5[
You will not be able to cover all of this in the time but these questions will help you get started.  
]

I will give you 20 minutes in your groups to do this and then we will discuss. 

---

## Critically reviewing the topics

The plots provide a way of viewing how the topics are assigned to the subreddits and we can use this to be critical about the topic modelling process. It is a glimpse of how the algorithms work that gives us a view of how the data is classified. We can already view this in the boxplots, but examining the parts of the structure allow us to gain a clearer view. Before we can visualise the data, we need to run a few conversion to reformat it and filter it correctly. 

Let us turn to the classifications within the topics. We will take the `comments_gamma` data frame* and group it by subreddit, then the id. 

```{r checkClassifications}
comment_classifications <- comments_gamma %>%
  group_by(subreddit, id) %>%
  # Select the topic with a higher probability per comment and subreddit.
  slice_max(gamma) %>%
  ungroup()

```


\* Remember, it's the one that contains probabilities of topics in document (in this case, by comment and subreddit)

---

.pull-left[
This is our newly created data frame 

```{r}
comment_classifications

nrow(comment_classifications)
```

]

.pull-right[
...and this is the original one:
```{r}
comments_gamma

nrow(comments_gamma)
```


]

We have the same columns but we have reduced the number of rows by selecting the maximum values per subreddit and comment (`id`).

---

We can look at the places where the subreddit that the topic is associated with does not match the subreddit. In looking at this, we can critically think about the way that the topic model is guessing which topic to which a piece of text belongs. 


```{r createSubreddits}
# Take the classifications, group by subreddit and topic. 
subreddit_topics <- comment_classifications %>%
  count(subreddit, topic) %>%
  group_by(subreddit) %>%
  top_n(1, n) %>%
  ungroup() %>%
  rename(consensus = subreddit) %>% 
  select(consensus, topic)


# Let's filter the subreddit classifications where it does not match consensus
# We'll join the subreddit and classifications together and filter where
# the subreddit and predicted subreddit do not match. 
comment_classifications %>%
  inner_join(subreddit_topics, by = "topic") %>%
  filter(subreddit != consensus)
```

\* Read about `ìnner_joins()` here: https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf (page 2)

---

The matrix here can be augmented, using augment to add the columns from the LDA analysis to the Document Term Matrix to get a better view on how the data is created. We separate the names that we originally created to identify the subreddits and comment ids. 

```{r byWord}
assignments <- augment(comments_lda, data = comments_dtm)

assignments <- assignments %>%
  separate(document, c("subreddit", "id"), 
           sep = "_", convert = TRUE) %>%
  inner_join(subreddit_topics, by = c(".topic" = "topic"))
head(assignments)
```


---

The top of the assignments is shown to help you understand what the data is. You can see the two topic assignments for the same word, which is the assignment by the topic model. It is not the easiest form to read, so we will visualise it. 

---

If you want to explore particular topic further, a simple tool has been written below for you. We can filter the data by a topic to identify the words and what they might come from. You can alter `id == ??=` to `term==???` to identify terms, if you want to filter it further. 


```{r findTopicLanguage} 
docs <- assignments %>%
  filter(id==262) %>%
  arrange(desc(count))
docs
```

---

We can visualise the way that the topics are constructed by creating a graph of the way that the topics are matched. This allows us to see how the machine is viewing the language that is being used. 


.panelset.sideways[
.panel[.panel-name[R Code]
```{r visualiseTopics, fig.show='hide'}
library(scales)

assignments %>%
  count(subreddit, consensus, wt = count) %>%
  mutate(across(c(subreddit, consensus), ~str_wrap(., 20))) %>%
  group_by(subreddit) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, subreddit, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "darkred", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Comment words were assigned to",
       y = "Comment words came from",
       fill = "% of assignments")
```
]
.panel[.panel-name[Plot]
![](`r knitr::fig_chunk("visualiseTopics", ".png")`)
]

]



How does this graph help interpret how the process is interpreting the data? How does it help identify places where language might be shared?

---

class: slide-primary

## Conclusion

In this lab, we build on the previous text analysis to identify the ways that machines might help us read and understand a text. TF-IDF uses ranking to identify the closeness of the documents and to determine their similarity. In contrast, topic modelling takes a statistical approach to the discovering statistical densities that it calls topics and groups the documents together in that way. As well as being used to explore the data by language, we can use the critical thinking from the previous sessions to think about how the methods presented here work, such as considering ranking or the use of probabilities. 

The questions that we should be raising concern the choices of methods and their affordances; what criteria are used to select, reduce and format the data; how does the method operate - that is, what does it use to create its result? 

The approaches presented here and in the previous two session focus on the documents presented to them, so they are platform agnostic. However, the platform limitations may come into play here, such as constraints on the language length or the type of language that might be used. Unlike the search engine week where we re-purposed the device (the search engine), these methods build on the device (the API) and incorporate it. In each week, we have altered the format of the data to the technique being used. In APIs, we convert JSON into CSVs or data frames. These are transformed into igraph objects in week 4 and then Gephi objects. In the last two sessions, we have transformed a text column into numeric values. When we work with digital methods, we need to be aware of these issues.  

---

layout: true

## Bibliography

---

Ben-David, Anat, and Oren Soffer. 'User Comments across Platforms and Journalistic Genres'. Information, Communication & Society 22, no. 12 (15 October 2019): 1810–29. 

Dobson, J. 2019. "Can an Algorithm be Disturbed? Machine Learning, Intrinsic Criticism, and the Digital Humanities" in Critical Digital Humanities: The Search for a Methodology, University of Illinois Press, Urbana, Chicago and Springfield

Rieder, B. 2020. “From Frequencies to Vectors” in Engines of Order: A Mechanology of Algorithmic Techniques. Amsterdam University Press, Amsterdam.

Rieder, B. 2020. “Interested Learning” in Engines of Order: A Mechanology of Algorithmic Techniques. Amsterdam University Press, Amsterdam.

Silge, J and Robinson, D. 2013. Text Mining with R. O'Reilly, Sebastopol.

---

### Topic Modelling

Blei, D.M., Ng, A.Y. and Jordan, M.I., 2003. Latent Dirichlet Allocation. The Journal of Machine Learning research, 3, pp.993-1022. <https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>

Buurma, R.S., 2015 The fictionality of topic modeling: Machine reading Anthony Trollope’s Barsetshire series. Big Data & Society 2:2.

Callaway, E, Turner, J, Stone, H, Halstrom, A. 2020.The Push and Pull of Digital Humanities: Topic Modeling the “What is digital humanities?” Genre. Digital Humanities Quarterly. 14: 1. 

Topic Modeling, Computing for the Social Sciences, <https://cfss.uchicago.edu/notes/topic-modeling/>

Intuitive Guide to Latent Dirichlet Allocation, <https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158>